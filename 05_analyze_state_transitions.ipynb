{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import _hmmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data munging functions\n",
    "\n",
    "def make_dfs(data_dir, first_n=1000):\n",
    "    dfs = [\n",
    "        (file, pd.read_csv(file)\n",
    "        .sort_values(\"epoch\")\n",
    "        # .sort_values(\"step\")\n",
    "        .reset_index(drop=True)\n",
    "        .head(first_n)\n",
    "        )  \n",
    "        for file in glob.glob(data_dir + \"*\")\n",
    "    ]\n",
    "    file_names, dfs = zip(*dfs)\n",
    "    return file_names, dfs\n",
    "\n",
    "def make_hmm_data(dfs, cols):\n",
    "    dfs = [df[cols] for df in dfs]\n",
    "    \n",
    "    data = np.vstack(\n",
    "        [np.apply_along_axis(zscore, 0, df.to_numpy()) for df in dfs]\n",
    "    )\n",
    "    return data\n",
    "\n",
    "def break_list_by_lengths(lst, lengths):\n",
    "    result = []\n",
    "    start_index = 0\n",
    "    \n",
    "    for length in lengths:\n",
    "        sublist = lst[start_index:start_index + length]\n",
    "        result.append(sublist)\n",
    "        start_index += length\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def munge_data(model_pth, data_dir, cols, n_components, first_n=1000):\n",
    "    '''\n",
    "    Convert raw CSV data into expected format for analysis\n",
    "    '''\n",
    "    file_names, dfs = make_dfs(data_dir, first_n=first_n)\n",
    "    data = make_hmm_data(dfs, cols)\n",
    "    lengths =  [len(df) for df in dfs]\n",
    "\n",
    "    with open(model_pth, 'rb') as f:\n",
    "        models = pickle.load(f)\n",
    "\n",
    "    model = models['best_models'][n_components-1]\n",
    "    print(model.score(data, lengths=lengths))\n",
    "    best_predictions = break_list_by_lengths(model.predict(data, lengths=lengths), lengths)\n",
    "    \n",
    "    return model, data, best_predictions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative computation functions\n",
    "\n",
    "def softmax_with_overflow(logits):\n",
    "    '''\n",
    "    log-sum-exp\n",
    "    '''\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "def find_i_followed_by_j(lst, i, j):\n",
    "    '''\n",
    "    Find transition in the estimated hidden state\n",
    "    '''\n",
    "    indexes = [index for index in range(len(lst) - 1) if lst[index] == i and lst[index + 1] == j]\n",
    "    return indexes\n",
    "\n",
    "def get_derivatives(X, model):\n",
    "    '''\n",
    "    Compute the derivative d/dz_t p(s_t = k | z_{1:t}) for the entire forward lattice.\n",
    "    '''\n",
    "    derivatives = []\n",
    "    \n",
    "    log_frameprob = model._compute_log_likelihood(X)\n",
    "    log_probij, fwdlattice = _hmmc.forward_log(\n",
    "                model.startprob_, model.transmat_, log_frameprob)\n",
    "    n_components = fwdlattice.shape[1] # can be computed another way\n",
    "    covars = [np.linalg.inv(model.covars_[i]) for i in range(n_components)]\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        derivatives_i = []\n",
    "        probs = softmax_with_overflow(fwdlattice[i]) \n",
    "        Z = np.sum([probs[j] * covars[j] @ (model.means_[j] - X[i]) for j in range(n_components)])\n",
    "        \n",
    "        for component in range(n_components):\n",
    "            derivatives_i.append(\n",
    "                covars[component] @ (model.means_[component] - X[i]) - Z \n",
    "            )\n",
    "            \n",
    "        derivatives.append(derivatives_i)\n",
    "\n",
    "    return derivatives\n",
    "\n",
    "def get_features_for_transition(model, data, best_predictions, lengths, phase_1, phase_2):\n",
    "    '''\n",
    "    For each time a transition (phase_1 -> phase_2) happens, compute the derivatives for each feature.\n",
    "    \n",
    "    This computation is slightly inefficient, in that it computes the entire forward lattice of derivatives.\n",
    "    In practice, this inefficiency doesn't seem to be an issue in terms of runtime.\n",
    "    '''\n",
    "    features = []\n",
    "    for (i, datum) in enumerate(break_list_by_lengths(data, lengths)):\n",
    "        preds = best_predictions[i]\n",
    "        indexes = find_i_followed_by_j(preds, phase_1, phase_2)\n",
    "        if indexes != []:\n",
    "            derivatives = np.array(get_derivatives(datum, model))\n",
    "            for idx in indexes:\n",
    "                features.append(derivatives[idx, phase_2])\n",
    "    return features\n",
    "\n",
    "def get_difference_bt_means(model, phase_1, phase_2):\n",
    "    return model.means_[phase_2] - model.means_[phase_1]\n",
    "\n",
    "def characterize_transition(model, data, best_predictions, cols, lengths, i, j):\n",
    "    '''\n",
    "    Compute the average derivative for each feature, sort features by highest absolute value\n",
    "    '''\n",
    "    features = get_features_for_transition(model, data, best_predictions, lengths, i, j)\n",
    "    print(f\"Number of times transition happened: {len(features)}\")\n",
    "    features = np.mean(features, axis=0)\n",
    "    order = np.argsort(np.abs(features))[::-1]\n",
    "    print(cols[order])\n",
    "\n",
    "    feature_changes = np.array(get_difference_bt_means(model, i, j))\n",
    "    print(feature_changes[order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338166.5589622997\n",
      "CHANGE 4 -> 1\n",
      "Number of times transition happened: 40\n",
      "['l1' 'mean_singular_value' 'var_singular_value' 'l2' 'var_w' 'median_w'\n",
      " 'spectral' 'trace' 'var_b' 'median_b' 'computational_sparsity' 'mean_b'\n",
      " 'mean_w' 'code_sparsity']\n",
      "[ 0.627  0.563  0.705  0.644  0.429 -0.938  0.842 -0.768 -0.02   0.346\n",
      " -1.338  0.137 -0.712 -1.825]\n",
      "CHANGE 1 -> 3\n",
      "Number of times transition happened: 40\n",
      "['mean_singular_value' 'l2' 'l1' 'var_w' 'var_singular_value' 'spectral'\n",
      " 'code_sparsity' 'median_w' 'var_b' 'computational_sparsity' 'median_b'\n",
      " 'mean_b' 'trace' 'mean_w']\n",
      "[ 0.755  0.767  0.762  0.716  0.723  0.751 -0.699 -0.859  0.063 -0.652\n",
      " -0.056 -0.376 -0.722 -0.81 ]\n",
      "CHANGE 3 -> 2\n",
      "Number of times transition happened: 40\n",
      "['l2' 'mean_singular_value' 'var_singular_value' 'var_w' 'median_w'\n",
      " 'computational_sparsity' 'median_b' 'var_b' 'mean_b' 'trace'\n",
      " 'code_sparsity' 'mean_w' 'spectral' 'l1']\n",
      "[ 0.804  0.825  0.774  0.86  -0.709 -0.57  -0.114  0.248 -0.801 -0.791\n",
      " -0.436 -0.788  0.73   0.807]\n",
      "CHANGE 2 -> 0\n",
      "Number of times transition happened: 40\n",
      "['l2' 'mean_singular_value' 'var_w' 'var_singular_value' 'l1' 'mean_w'\n",
      " 'median_w' 'spectral' 'computational_sparsity' 'mean_b' 'trace' 'var_b'\n",
      " 'median_b' 'code_sparsity']\n",
      "[ 0.726  0.755  0.818  0.781  0.736 -0.653 -0.548  0.722 -0.218 -1.172\n",
      " -0.688  0.376 -0.009 -0.248]\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "\n",
    "# CIFAR 100\n",
    "cols = np.array([\n",
    "    \"l1\",\n",
    "    \"l2\",\n",
    "    \"trace\",\n",
    "    \"spectral\",\n",
    "    \"code_sparsity\",\n",
    "    \"computational_sparsity\",\n",
    "    \"mean_singular_value\",\n",
    "    \"var_singular_value\",\n",
    "    \"mean_w\",\n",
    "    \"median_w\",\n",
    "    \"var_w\",\n",
    "    \"mean_b\",\n",
    "    \"median_b\",\n",
    "    \"var_b\",\n",
    "])\n",
    "\n",
    "model_pth='/scratch/myh2014/modeling-training/data/model_selection/32/cifar100_v3/True_True/--base.pkl'\n",
    "data_dir='/scratch/myh2014/modeling-training/data/training_runs/cifar100_v3/True_True/'\n",
    "n_components=5\n",
    "\n",
    "model, data, best_predictions, lengths = munge_data(model_pth, data_dir, cols, n_components)\n",
    "\n",
    "print('CHANGE 4 -> 1')\n",
    "\n",
    "characterize_transition(model, data, best_predictions, cols, lengths, 4, 1)\n",
    "\n",
    "print('CHANGE 1 -> 3')\n",
    "\n",
    "characterize_transition(model, data, best_predictions, cols, lengths, 1, 3)\n",
    "\n",
    "print('CHANGE 3 -> 2')\n",
    "\n",
    "characterize_transition(model, data, best_predictions, cols, lengths, 3, 2)\n",
    "\n",
    "print('CHANGE 2 -> 0')\n",
    "\n",
    "characterize_transition(model, data, best_predictions, cols, lengths, 2, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
